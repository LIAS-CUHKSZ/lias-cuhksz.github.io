---
layout: post
title:  "Linear Convergence Analysis of Single-loop Algorithm for Bilevel Optimization via Small-gain Theorem"
date:   2024-12-06 23:46:07
categories: research
author: "Jianhui Li"
published: true
sidebar:  false
permalink: /lca/
image: /img/posts/2024-12-06-lca/1.png
title_image: None
link-new-tab: true
tags: Optimization
# hero_image: /img/posts/2025-04-10-rayfronts/rayfronts-teaser.gif
---

# Abstract

Bilevel optimization has gained considerable attention due to its broad applicability across various fields. While several studies have investigated the convergence rates in the strongly-convex-strongly-convex (SC-SC) setting, no prior work has proven that a single-loop algorithm can achieve linear convergence. This paper employs a small-gain theorem in {robust control theory} to demonstrate that a single-loop algorithm based on the implicit function theorem attains a linear convergence rate of    , where   is specified in Theorem 3. Specifically, We model the algorithm as a dynamical system by identifying its two interconnected components: the controller (the gradient or approximate gradient functions) and the plant (the update rule of variables). We prove that each component exhibits a bounded gain and that, with carefully designed step sizes, their cascade accommodates a product gain strictly less than one. Consequently, the overall algorithm can be proven to achieve a linear convergence rate, as guaranteed by the small-gain theorem. The gradient boundedness assumption adopted in the single-loop algorithm (\cite{hong2023two, chen2022single}) is replaced with a gradient Lipschitz assumption in Assumption 2.2. To the best of our knowledge, this work is first-known result on linear convergence for a single-loop algorithm. 

**link:**[https://arxiv.org/abs/2412.00659](https://arxiv.org/abs/2412.00659)